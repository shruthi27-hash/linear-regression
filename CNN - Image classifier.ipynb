{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1nnaVG7H_9k2fDcvXyOqpWqqw7Di3qG6p","timestamp":1744736308473}],"authorship_tag":"ABX9TyO4Ph9jp2ySzmBTauswOUXQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":699},"id":"VcCbH2srUjdu","executionInfo":{"status":"error","timestamp":1744735639641,"user_tz":-330,"elapsed":423870,"user":{"displayName":"Shruthi Balasubramanian","userId":"00594072002931433277"}},"outputId":"308da3d3-ce3c-4d2b-9d7f-cb23579e489b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 41ms/step - accuracy: 0.2599 - loss: 1.9741 - val_accuracy: 0.4951 - val_loss: 1.4295\n","Epoch 2/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 41ms/step - accuracy: 0.4456 - loss: 1.5135 - val_accuracy: 0.5595 - val_loss: 1.2395\n","Epoch 3/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 40ms/step - accuracy: 0.4981 - loss: 1.3882 - val_accuracy: 0.5935 - val_loss: 1.1467\n","Epoch 4/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 42ms/step - accuracy: 0.5254 - loss: 1.3104 - val_accuracy: 0.6277 - val_loss: 1.0678\n","Epoch 5/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 40ms/step - accuracy: 0.5508 - loss: 1.2274 - val_accuracy: 0.6372 - val_loss: 1.0585\n"]},{"output_type":"error","ename":"TypeError","evalue":"Image.__init__() got an unexpected keyword argument 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bcb89d1f02c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m demo = gr.Interface(\n\u001b[1;32m     52\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassify_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Upload Image (CIFAR-10 style)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ğŸ§  CNN Image Classifier (CIFAR-10)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Image.__init__() got an unexpected keyword argument 'shape'"]}],"source":["# Install Gradio\n","!pip install gradio --quiet\n","\n","import gradio as gr\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load CIFAR-10 dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","y_train_cat = to_categorical(y_train, 10)\n","y_test_cat = to_categorical(y_test, 10)\n","\n","# Class names\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# CNN model\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dropout(0.5),\n","    Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model (small epochs to keep things light)\n","model.fit(x_train, y_train_cat, epochs=5, validation_data=(x_test, y_test_cat))\n","\n","# Prediction function\n","def classify_image(img):\n","    img_resized = tf.image.resize(img, [32, 32])\n","    img_array = np.expand_dims(img_resized / 255.0, axis=0)\n","    pred = model.predict(img_array)[0]\n","    top_class = class_names[np.argmax(pred)]\n","    confidence = np.max(pred) * 100\n","    return f\"ğŸ§  Predicted: {top_class} ({confidence:.2f}%)\"\n","\n","# Gradio UI\n","demo = gr.Interface(\n","    fn=classify_image,\n","    inputs=gr.Image(shape=(32, 32), label=\"Upload Image (CIFAR-10 style)\"),\n","    outputs=\"text\",\n","    title=\"ğŸ§  CNN Image Classifier (CIFAR-10)\",\n","    description=\"Upload a 32x32 image (or close to it) to classify it using a CNN trained on CIFAR-10.\"\n",")\n","\n","demo.launch()\n","\n"]},{"cell_type":"code","source":["# Install Gradio\n","!pip install gradio --quiet\n","\n","# Import libraries\n","import gradio as gr\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","\n","# Load CIFAR-10 dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","y_train_cat = to_categorical(y_train, 10)\n","y_test_cat = to_categorical(y_test, 10)\n","\n","# Class names for CIFAR-10\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# Build CNN model\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dropout(0.5),\n","    Dense(10, activation='softmax')\n","])\n","\n","# Compile model\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model (light training for quick demo)\n","model.fit(x_train, y_train_cat, epochs=5, validation_data=(x_test, y_test_cat))\n","\n","# Prediction function for Gradio\n","def classify_image(img):\n","    img_resized = tf.image.resize(img, [32, 32])\n","    img_array = np.expand_dims(img_resized / 255.0, axis=0)\n","    pred = model.predict(img_array)[0]\n","    top_class = class_names[np.argmax(pred)]\n","    confidence = np.max(pred) * 100\n","    return f\"ğŸ§  Predicted: {top_class} ({confidence:.2f}%)\"\n","\n","# Gradio UI\n","demo = gr.Interface(\n","    fn=classify_image,\n","    inputs=gr.Image(type=\"numpy\", label=\"Upload Image (CIFAR-10 style)\"),\n","    outputs=\"text\",\n","    title=\"ğŸ§  CNN Image Classifier (CIFAR-10)\",\n","    description=\"Upload or draw an image to classify it into one of the 10 CIFAR-10 categories.\"\n",")\n","\n","# Launch the app\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":819},"id":"41pAFk_iWr-K","executionInfo":{"status":"ok","timestamp":1744736173484,"user_tz":-330,"elapsed":420526,"user":{"displayName":"Shruthi Balasubramanian","userId":"00594072002931433277"}},"outputId":"35ab4ade-954d-4bb6-b0a8-f0722e1d31db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 43ms/step - accuracy: 0.2935 - loss: 1.8857 - val_accuracy: 0.5406 - val_loss: 1.3035\n","Epoch 2/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 41ms/step - accuracy: 0.4959 - loss: 1.4030 - val_accuracy: 0.5861 - val_loss: 1.1708\n","Epoch 3/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 42ms/step - accuracy: 0.5451 - loss: 1.2740 - val_accuracy: 0.6337 - val_loss: 1.0474\n","Epoch 4/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 42ms/step - accuracy: 0.5795 - loss: 1.1834 - val_accuracy: 0.6467 - val_loss: 1.0131\n","Epoch 5/5\n","\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 41ms/step - accuracy: 0.6000 - loss: 1.1249 - val_accuracy: 0.6585 - val_loss: 0.9911\n","It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://21c9bb902047c758c6.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://21c9bb902047c758c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":2}]}]}